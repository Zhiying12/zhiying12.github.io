---
title: 'Re-evaluation on MultiPaxos under partial network partition from 
the OmniPaxos paper'
date: 2023-06-28
permalink: /posts/2023/06/blog-post-1/
---

> TL;DR: OmniPaxos proposes some changes in MultiPaxos to deal with partial
> network partitions, where classic MultiPaxos and Raft lead to deadlocks or
> livelocks. However, our MultiPaxos implementation can easily elect a new
> stable leader under the network partition.

This paper appeared in EuroSys 23. It proposed a resilient approach, called
OmniPaxos, to deal with partial network partition. The partial network partition
refers to a scenario where two servers are unreachable from each other but
connected by a third server. The authors argue that, under the partial network
connectivity circumstance, traditional RSM systems (like Multi-Paxos, Raft, VR)
may lead to performance degradation and even stop progressing because of
deadlocks or livelocks.

## What is partial network partition?

![Example](https://raw.githubusercontent.com/Zhiying12/zhiying12.github.io/master/images/post-06%3A28-partition.png "Example")

This paper identifies and focuses on three types of partial network connectivity
that can lead to livelocks in existing RSM systems. All three scenarios are 
shown in the graph borrowed from the paper.

- ***Quorum-loss scenario***. Given an initial cluster of five fully-connected 
  servers , where Server C is the stable leader, the quorum-loss scenario may arise when all
servers are connected to Server A only and disconnected from the rest. In this
case, Server B, D, and E cannot become the leader because they only receive
votes from Server A and itself at most, which is less than the majority
requirement, while Server A will not start the leader election because it can
still receive heartbeats from the existing leader C. \
Though this claim is not fully valid, the authors argue that, except for Raft, 
the replication progress will be stalled because no functional leader will be 
elected. (However, we will show that MultiPaxos can handle this scenario as 
well in the later section).

* ***Constrained Election Scenario***. It is similar to the previous scenarios, but 
this time, Server A is disconnected from C. If A does not contain the latest log, for
Raft, VR, and Zab, A cannot become the leader, because these protocols have log
requirements for the leader election. Similarly, the replication progress will
be suspended because of no leader. 

- ***Chained Scenarios***. Given a cluster of three fully-connected servers, 
where Server B is the leader. If B and C become unreachable from each other, the
chained scenario may occur. C will start the leader election and become the
leader. When Server B learns about the new leader via A, it will trigger another
round of leader election, because B will not receive any heartbeats from leader 
C. Similarly, when B becomes leader, C may start an election again. With 
repeated leader election, the cluster will lead to a livelock situation. 
This is noted as the Cloudfare outage case. 

## How OmniPaxos deals with it?

This paper proposes a solution by introducing a concept called quorum-connected
servers for leader election and decoupling leader election and log replication.
> A quorum-connected server maintains connection to at least the majority of servers. 

All servers exchange the information of what servers are quorum-connected
periodically. When the leader election happens, servers only vote for the 
candidate with the highest ballot that is quorum-connected. Additionally,
OmniPaxos separates and modularizes leader election, log replication, and
reconfiguration, eliminating the requirement of the log for the leader election.
With these two main changes, OmniPaxos ensures a stable leader can always be
elected under the three partial network partition scenarios above.

## Can MultiPaxos survive under the quorum-loss scenario?

![MP](https://raw.githubusercontent.com/Zhiying12/zhiying12.github.io/master/images/post-06%3A28-mp.png "MP")

Despite the claim from the paper, we disagree with the claim and evaluation
results in the paper that MultiPaxos leads to deadlock in the case of the
quorum-loss scenario. In our MultiPaxos implementation, it can easily elect 
a new stable leader. As shown in the figure, there are several steps as follows.

1. When this quorum-loss scenario happens, all servers, except for Server A, 
   lose the connection with C.
2. Then Server B, D, or E will trigger the leader election with a higher ballot 
   and send prepare (P1) requests to A. Because of the higher ballot,
   Server A will give a promise to one of them and update its own ballot.
   Consequently, Server A will no longer respond to the heartbeat from the old
   leader C. 
3. In the meantime, Server B, D, and E cannot become a new leader, A will not
   receive any heartbeats from the new leader, resulting in a timeout and
   another round of leader election initialized by A. 
4. A will be the new leader, and the system will return to normal and make 
   progress.

## Reevaluation on MultiPaxos with quorum-loss scenario

![Result](https://raw.githubusercontent.com/Zhiying12/zhiying12.github.io/master/images/post-06%3A28-result.png "Result")

We evaluated our Multi-Paxos implementation with the same experiment setting
from the paper. We deployed five MultiPaxos instances on AWS m5.2xlarge
machines (8 cores and 32 GiB RAM). We repeated experiments with timeout of 
50, 500, and 5000 ms.

Although our implementation does not include any significant optimizations, i.e. 
no optimization on leader election and failure detectors, the result shows that 
the down-time is even better than OmniPaxos. It contradicts with the claim that 
Multi-Paxos would cause deadlock in the quorum-loss scenario. Additionally, 
Multi-Paxos does not need to disseminate the information of quorum connectivity, 
which can decrease a larger number of messages exchanged among peers than 
Omnipaxos.

Inspired by the raft paper [] and etcd v3.5 [], we also introduced CheckQuorum
to MultiPaxos. When the leader no longer holds a quorum of connections, it will
become a follower again and allow leader elections. We examine it with the same
setting and denote it as MultiPaxos+CQ in the figure. It shows that its
down-time is further improved, especially with the timeout of 5000ms. Normally,
Server A can be elected as the new leader after the duration of around 2
timeouts (1 for initial timeout of Server B, D, E, 1 for timeout of Server A).
Now, with CheckQuorum, the old leader C actively stops broadcasting heartbeats,
then A gets a chance to start leader election immediately. Thus the down-time
may be reduced to around 1 timeout. When the timeout is much longer, its effect
becomes more significant.

## What about Chained scenarios?

![chaiined](https://raw.githubusercontent.com/Zhiying12/zhiying12.github.io/master/images/post-06%3A28-chained.png "chained")

Unfortunately, when we examine MultiPaxos under Chained scenarios, it does show
that MultiPaxos experiences livelocks in every run. Therefore, we need a
solution to Chained scenarios for MultiPaxos. While OmniPaxos can find a stable
leader, there are still some limitations. As we can see from the figure above,
the new leader C can only connect with A, which means that data is replicated
only on Server A and C. When the partition lasts too long, Sever B will be a
laggard. It not only stops log compaction, but also increases more overhead for
log recovery on B when the partition is removed. Moreover, Server B is not aware
of the new leader, so it needs extra coordination to direct client requests from
B to C.

However, if we can elect Sever A as the new leader in the first place, A can
replicate data to all peers, which saves spaces for in-memory data. In addition,
even after the network resumes, Sever A is still a stable leader, it does not
need any new leader elections or extra coordination.

We propose a simple approach based on the CheckQuorum mechanism. When the leader
notices that some heartbeat requests become undeliverable, it can stop sending
out heartbeats to allow a leader election. In this case, Server A will be the
new leader, which is the optimal choice for Chained scenarios.

## Further discussion

The main limitation of this approach is that peers are not fully sure whether a
network partition is partially disconnected (partial partition) or totally
disconnected (full partition). When the replication group is more than 3, such
as 5, we need to figure out which node is better as the new leader.

We can use the information of quorum-connected servers. When the network
partition is placed, the server connected with the most quorum-connected servers
is supposed to have higher priority to start the leader election, i.e., be a new
leader. With the most connection to quorum-connected servers, it can suppress
most unnecessary leader elections, so that it is more possible to avoid live
locks.


